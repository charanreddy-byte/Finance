{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb78794-8c44-486d-8b77-1d0d4d168f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading history for training...\n",
      "History rows: 4377\n",
      "[build] total rows=4377, valid=4335, bad=0\n",
      "Built X,y shapes: (4335, 64) (4335, 12)\n",
      "Loaded model and scaler from disk.\n",
      "Realtime loop (Ctrl+C to stop).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_3744\\692399001.py:303: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  last_price = float(df_now.iloc[-1][\"Close\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[2025-09-21 09:08:22 UTC] Last close = 140.9000\n",
      " t+01: ret=+0.000% -> price ≈ 140.9000\n",
      " t+02: ret=+0.059% -> price ≈ 140.9828\n",
      " t+03: ret=+0.089% -> price ≈ 141.0257\n",
      " t+04: ret=+0.286% -> price ≈ 141.3035\n",
      " t+05: ret=+0.400% -> price ≈ 141.4629\n",
      " t+06: ret=-0.021% -> price ≈ 140.8711\n",
      " t+07: ret=+0.828% -> price ≈ 142.0670\n",
      " t+08: ret=+0.120% -> price ≈ 141.0691\n",
      " t+09: ret=+0.364% -> price ≈ 141.4135\n",
      " t+10: ret=+0.913% -> price ≈ 142.1858\n",
      " t+11: ret=+0.561% -> price ≈ 141.6909\n",
      " t+12: ret=+0.589% -> price ≈ 141.7304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored from cffi callback <function buffer_callback at 0x0000019F9525E660>:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chara\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\curl_cffi\\curl.py\", line 100, in buffer_callback\n",
      "    @ffi.def_extern()\n",
      "KeyboardInterrupt: \n",
      "\n",
      "1 Failed download:\n",
      "['ASHOKLEY.BO']: RequestException('Failed to perform, curl: (23) Failure writing output to destination, passed 13 returned 0. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough recent data; sleeping...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "predict.py (robust final)\n",
    "\n",
    "- Uses 5m bars (60d) by default for long history.\n",
    "- Defensive feature-building and multi-step return targets.\n",
    "- Trains MultiOutputRegressor on returns and enters realtime loop.\n",
    "- Robust access to last_price using .iloc and try/except so the loop won't crash.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import traceback\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# --------------- CONFIG ----------------\n",
    "TICKER = \"ASHOKLEY.BO\"\n",
    "INTERVAL = \"5m\"\n",
    "PERIOD = \"60d\"\n",
    "WINDOW = 30\n",
    "K = 12\n",
    "REFRESH_SECONDS = 60\n",
    "MODEL_FILE = \"model_returns_k.joblib\"\n",
    "SCALER_FILE = \"scaler_returns_k.joblib\"\n",
    "LOG_FILE = \"logs/predictions_returns.csv\"\n",
    "DEBUG_BAD = \"debug_bad_rows.npy\"\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "# ---------------------------------------\n",
    "\n",
    "def fetch_history(ticker: str, interval: str, period: str) -> pd.DataFrame:\n",
    "    def process_df(df):\n",
    "        if df is None or df.empty:\n",
    "            return pd.DataFrame()\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        cols = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"] if c in df.columns]\n",
    "        df = df.loc[:, cols]\n",
    "        for c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df.dropna(inplace=True)\n",
    "        return df\n",
    "\n",
    "    try:\n",
    "        df = yf.download(tickers=ticker, interval=interval, period=period,\n",
    "                         progress=False, threads=False, auto_adjust=False)\n",
    "        return process_df(df)\n",
    "    except Exception as e:\n",
    "        print(\"Fetch error:\", e)\n",
    "        # fallback minimal handling for 1m\n",
    "        if interval == \"1m\":\n",
    "            try:\n",
    "                df = yf.download(tickers=ticker, interval=\"1m\", period=\"7d\",\n",
    "                                 progress=False, threads=False, auto_adjust=False)\n",
    "                return process_df(df)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    df = yf.download(tickers=ticker, interval=\"5m\", period=period,\n",
    "                                     progress=False, threads=False, auto_adjust=False)\n",
    "                    return process_df(df)\n",
    "                except Exception as e3:\n",
    "                    print(\"All fetch fallbacks failed:\", e3)\n",
    "                    return pd.DataFrame()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _is_sequence_like(x):\n",
    "    if isinstance(x, (list, tuple, np.ndarray, pd.Series)):\n",
    "        return True\n",
    "    try:\n",
    "        import collections.abc\n",
    "        if isinstance(x, collections.abc.Sequence) and not isinstance(x, (str, bytes)):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def _maybe_extract_scalar(x):\n",
    "    try:\n",
    "        if isinstance(x, pd.Series):\n",
    "            a = x.values\n",
    "        else:\n",
    "            a = np.asarray(x)\n",
    "        if a.ndim == 0:\n",
    "            return a.item()\n",
    "        if a.size == 1:\n",
    "            return a.flatten()[0]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return x\n",
    "\n",
    "def build_X_y_returns_safe(df: pd.DataFrame, window: int, k: int):\n",
    "    closes = df[\"Close\"].values\n",
    "    volumes = df[\"Volume\"].values\n",
    "    idx = df.index\n",
    "    n = len(df)\n",
    "    expected_len = 2 * window + 4\n",
    "\n",
    "    X_rows = []\n",
    "    y_rows = []\n",
    "    bad_rows = []\n",
    "\n",
    "    for i in range(window, n - k):\n",
    "        try:\n",
    "            past_closes = closes[i - window:i]\n",
    "            if len(past_closes) != window:\n",
    "                bad_rows.append((i, \"past_closes_len\", len(past_closes)))\n",
    "                continue\n",
    "            past_returns = (past_closes[1:] - past_closes[:-1]) / (past_closes[:-1] + 1e-9)\n",
    "            if len(past_returns) != (window - 1):\n",
    "                bad_rows.append((i, \"past_returns_len\", len(past_returns)))\n",
    "                continue\n",
    "\n",
    "            feats = []\n",
    "            feats.extend(list(past_closes))\n",
    "            feats.extend(list(past_returns))\n",
    "            feats.append(np.mean(past_closes))\n",
    "            feats.append(np.std(past_closes))\n",
    "            feats.append(volumes[i - 1])\n",
    "            dt = idx[i]\n",
    "            minute_of_day = dt.hour * 60 + dt.minute\n",
    "            feats.append(np.sin(2 * np.pi * minute_of_day / 1440))\n",
    "            feats.append(np.cos(2 * np.pi * minute_of_day / 1440))\n",
    "\n",
    "            # ensure scalar elements\n",
    "            safe_feats = []\n",
    "            malformed = False\n",
    "            for j, el in enumerate(feats):\n",
    "                if _is_sequence_like(el):\n",
    "                    el2 = _maybe_extract_scalar(el)\n",
    "                    if _is_sequence_like(el2):\n",
    "                        bad_rows.append((i, f\"elem_multi_at_{j}\", type(el2).__name__))\n",
    "                        malformed = True\n",
    "                        break\n",
    "                    else:\n",
    "                        safe_feats.append(el2)\n",
    "                else:\n",
    "                    safe_feats.append(el)\n",
    "\n",
    "            if malformed:\n",
    "                continue\n",
    "\n",
    "            # cast to float array\n",
    "            try:\n",
    "                feat_arr = np.asarray(safe_feats, dtype=float)\n",
    "            except Exception as e:\n",
    "                bad_rows.append((i, f\"cast_error:{e}\", str(safe_feats[:8])))\n",
    "                continue\n",
    "\n",
    "            if feat_arr.ndim != 1 or feat_arr.shape[0] != expected_len:\n",
    "                bad_rows.append((i, f\"bad_shape:{feat_arr.shape}\", feat_arr.shape))\n",
    "                continue\n",
    "\n",
    "            future = closes[i:i+k]\n",
    "            if len(future) != k:\n",
    "                bad_rows.append((i, \"future_len\", len(future)))\n",
    "                continue\n",
    "\n",
    "            returns_k = (future - closes[i]) / (closes[i] + 1e-9)\n",
    "            if np.any(np.isnan(returns_k)):\n",
    "                bad_rows.append((i, \"target_nan\", returns_k))\n",
    "                continue\n",
    "\n",
    "            # append as 2D rows to stack safely later\n",
    "            X_rows.append(feat_arr.reshape(1, -1))\n",
    "            y_rows.append(np.asarray(returns_k, dtype=float).reshape(1, k))\n",
    "\n",
    "        except Exception as ex:\n",
    "            bad_rows.append((i, f\"exception:{ex}\", traceback.format_exc()))\n",
    "            continue\n",
    "\n",
    "    # reporting\n",
    "    print(f\"[build] total rows={n}, valid={len(X_rows)}, bad={len(bad_rows)}\")\n",
    "    if bad_rows:\n",
    "        print(\"Examples of bad rows (up to 8):\")\n",
    "        for b in bad_rows[:8]:\n",
    "            print(\" \", b[0], \"-\", b[1], \"-\", str(b[2])[:160])\n",
    "        try:\n",
    "            np.save(DEBUG_BAD, bad_rows, allow_pickle=True)\n",
    "            print(f\"Wrote bad rows to {DEBUG_BAD}\")\n",
    "        except Exception as e:\n",
    "            print(\"Could not save debug bad rows:\", e)\n",
    "\n",
    "    if len(X_rows) == 0:\n",
    "        return np.empty((0, expected_len)), np.empty((0, k))\n",
    "\n",
    "    X = np.vstack(X_rows)\n",
    "    y = np.vstack(y_rows)\n",
    "    return X, y\n",
    "\n",
    "def train_and_save(X, y):\n",
    "    print(\"Training model: shapes X,y =\", X.shape, y.shape)\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise RuntimeError(f\"Sanity check failed: X samples={X.shape[0]} y samples={y.shape[0]}\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "    base = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3)\n",
    "    model = MultiOutputRegressor(base, n_jobs=-1)\n",
    "    model.fit(X_train_s, y_train)\n",
    "    y_pred = model.predict(X_val_s)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    print(f\"Validation MAE (returns avg): {mae:.6f}\")\n",
    "    joblib.dump(model, MODEL_FILE)\n",
    "    joblib.dump(scaler, SCALER_FILE)\n",
    "    print(\"Saved model & scaler.\")\n",
    "    return model, scaler\n",
    "\n",
    "def load_if_present():\n",
    "    if os.path.exists(MODEL_FILE) and os.path.exists(SCALER_FILE):\n",
    "        model = joblib.load(MODEL_FILE)\n",
    "        scaler = joblib.load(SCALER_FILE)\n",
    "        print(\"Loaded model and scaler from disk.\")\n",
    "        return model, scaler\n",
    "    return None, None\n",
    "\n",
    "def make_feature_vector_from_recent(df_recent, window):\n",
    "    if len(df_recent) < window:\n",
    "        return None\n",
    "    closes = df_recent[\"Close\"].values\n",
    "    volumes = df_recent[\"Volume\"].values\n",
    "    past_closes = closes[-window:]\n",
    "    past_returns = (past_closes[1:] - past_closes[:-1]) / (past_closes[:-1] + 1e-9)\n",
    "    feats = []\n",
    "    feats.extend(list(past_closes))\n",
    "    feats.extend(list(past_returns))\n",
    "    feats.append(np.mean(past_closes))\n",
    "    feats.append(np.std(past_closes))\n",
    "    feats.append(volumes[-1])\n",
    "    dt = df_recent.index[-1]\n",
    "    minute_of_day = dt.hour * 60 + dt.minute\n",
    "    feats.append(np.sin(2 * np.pi * minute_of_day / 1440))\n",
    "    feats.append(np.cos(2 * np.pi * minute_of_day / 1440))\n",
    "    # sanitize elements\n",
    "    safe_feats = []\n",
    "    for el in feats:\n",
    "        if _is_sequence_like(el):\n",
    "            el2 = _maybe_extract_scalar(el)\n",
    "            if _is_sequence_like(el2):\n",
    "                return None\n",
    "            safe_feats.append(el2)\n",
    "        else:\n",
    "            safe_feats.append(el)\n",
    "    try:\n",
    "        return np.asarray(safe_feats, dtype=float).reshape(1, -1)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def log_preds(ts, ticker, preds_returns, last_price):\n",
    "    row = {\"timestamp\": ts, \"ticker\": ticker, \"last_price\": float(last_price)}\n",
    "    for i, r in enumerate(preds_returns, start=1):\n",
    "        row[f\"pred_ret_t+{i}\"] = float(r)\n",
    "        row[f\"pred_price_t+{i}\"] = float(last_price * (1 + r))\n",
    "    df = pd.DataFrame([row])\n",
    "    header = not os.path.exists(LOG_FILE)\n",
    "    df.to_csv(LOG_FILE, mode=\"a\", header=header, index=False)\n",
    "\n",
    "def main():\n",
    "    print(\"Downloading history for training...\")\n",
    "    df_hist = fetch_history(TICKER, INTERVAL, PERIOD)\n",
    "    if df_hist.empty:\n",
    "        print(\"No history returned. Exiting.\")\n",
    "        return\n",
    "    print(\"History rows:\", len(df_hist))\n",
    "\n",
    "    X, y = build_X_y_returns_safe(df_hist, WINDOW, K)\n",
    "    print(\"Built X,y shapes:\", X.shape, y.shape)\n",
    "    if X.size == 0:\n",
    "        print(\"No valid samples after cleaning. Exiting.\")\n",
    "        return\n",
    "\n",
    "    model, scaler = load_if_present()\n",
    "    if model is None or scaler is None:\n",
    "        model, scaler = train_and_save(X, y)\n",
    "\n",
    "    print(\"Realtime loop (Ctrl+C to stop).\")\n",
    "    try:\n",
    "        while True:\n",
    "            df_now = fetch_history(TICKER, INTERVAL, \"2d\")\n",
    "            if df_now.empty or len(df_now) < WINDOW:\n",
    "                print(\"Not enough recent data; sleeping...\")\n",
    "                time.sleep(REFRESH_SECONDS)\n",
    "                continue\n",
    "\n",
    "            # Build feature vector safely\n",
    "            fv = make_feature_vector_from_recent(df_now, WINDOW)\n",
    "            if fv is None:\n",
    "                print(\"Could not build safe feature vector; sleeping...\")\n",
    "                time.sleep(REFRESH_SECONDS)\n",
    "                continue\n",
    "\n",
    "            # Safely get last price using iloc and try/except to avoid rare pandas oddities\n",
    "            try:\n",
    "                last_price = float(df_now.iloc[-1][\"Close\"])\n",
    "            except Exception as e:\n",
    "                print(\"Warning: failed to read last_price (skipping this iteration):\", e)\n",
    "                time.sleep(REFRESH_SECONDS)\n",
    "                continue\n",
    "\n",
    "            # Predict\n",
    "            try:\n",
    "                fv_s = scaler.transform(fv)\n",
    "                preds = model.predict(fv_s)[0]  # length K\n",
    "            except Exception as e:\n",
    "                print(\"Warning: model prediction failed this cycle:\", e)\n",
    "                time.sleep(REFRESH_SECONDS)\n",
    "                continue\n",
    "\n",
    "            now = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"[{now}] Last close = {last_price:.4f}\")\n",
    "            for i, r in enumerate(preds, start=1):\n",
    "                p_price = last_price * (1 + r)\n",
    "                print(f\" t+{i:02d}: ret={r*100:+.3f}% -> price ≈ {p_price:.4f}\")\n",
    "            log_preds(now, TICKER, preds, last_price)\n",
    "            time.sleep(REFRESH_SECONDS)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped by user. Exiting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d9b98-54a4-44ad-912d-4d0599f50587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7255d8-fecd-4132-9f05-a96408a1faf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 10y of 1d data for ASHOKLEY.BO...\n",
      "Successfully fetched 2468 rows of data.\n",
      "Building features and targets...\n",
      "Built X, y shapes: (2387, 35), (2387, 3)\n",
      "Training new model...\n",
      "  - Horizon 1-day => Validation MAE: 0.015818 | R²: -0.0789\n",
      "  - Horizon 5-day => Validation MAE: 0.039251 | R²: -0.3788\n",
      "  - Horizon 21-day => Validation MAE: 0.115511 | R²: -2.1610\n",
      "Saved model to model_ashokley.bo_dwm.joblib and scaler to scaler_ashokley.bo_dwm.joblib.\n",
      "\n",
      "Fetching latest data for prediction...\n",
      "Fetching 86d of 1d data for ASHOKLEY.BO...\n",
      "Successfully fetched 85 rows of data.\n",
      "\n",
      "==================================================\n",
      "PREDICTION for ASHOKLEY.BO @ 2025-09-21 09:35:27 UTC\n",
      "Based on last close price: 140.90\n",
      "==================================================\n",
      " Daily           | Horizon:  1 days | Return: +1.16% | Predicted Price: 142.54\n",
      " Weekly (5d)     | Horizon:  5 days | Return: +3.88% | Predicted Price: 146.36\n",
      " Monthly (21d)   | Horizon: 21 days | Return: -2.39% | Predicted Price: 137.53\n",
      "Prediction logged to logs/predictions_dwm.csv\n",
      "\n",
      "--- Final Predictions ---\n",
      "Daily Predicted Close Price: 142.54\n",
      "Weekly Predicted Close Price: 146.36\n",
      "Monthly Predicted Close Price: 137.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\88424051.py:250: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Based on last close price: {float(last_close):.2f}\", flush=True)\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\88424051.py:256: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  pred_price = float(last_close) * (1 + ret)\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\88424051.py:259: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  log_prediction(now_utc, TICKER, pred_returns, float(last_close))\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\88424051.py:262: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Daily Predicted Close Price: {float(last_close) * (1 + pred_returns[0]):.2f}\", flush=True)\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\88424051.py:263: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Weekly Predicted Close Price: {float(last_close) * (1 + pred_returns[1]):.2f}\", flush=True)\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\88424051.py:264: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Monthly Predicted Close Price: {float(last_close) * (1 + pred_returns[2]):.2f}\", flush=True)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "predict_dwm.py (Daily, Weekly, Monthly)\n",
    "\n",
    "- Uses daily bars (\"1d\") for long-term history.\n",
    "- Creates features from a look-back window and targets for 1-day, 5-day (weekly), and 21-day (monthly) returns.\n",
    "- Trains a MultiOutputRegressor to predict these three horizons simultaneously.\n",
    "- Runs once to generate the latest prediction and then exits.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# --------------- CONFIG ----------------\n",
    "TICKER = \"ASHOKLEY.BO\"\n",
    "INTERVAL = \"1d\"      # Changed to daily data\n",
    "PERIOD = \"10y\"       # Changed to a longer period for more context\n",
    "WINDOW = 60          # Look-back window in days\n",
    "HORIZONS = [1, 5, 21] # Prediction horizons: 1-day, 5-day (1 week), 21-day (1 month)\n",
    "PAST_RETURNS_LENGTH = 30 # Define a fixed length for past returns feature\n",
    "\n",
    "# --- File Paths ---\n",
    "MODEL_FILE = f\"model_{TICKER.lower()}_dwm.joblib\"\n",
    "SCALER_FILE = f\"scaler_{TICKER.lower()}_dwm.joblib\"\n",
    "LOG_FILE = \"logs/predictions_dwm.csv\"\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "# ---------------------------------------\n",
    "\n",
    "def fetch_history(ticker: str, interval: str, period: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetches and cleans historical market data.\"\"\"\n",
    "    print(f\"Fetching {period} of {interval} data for {ticker}...\", flush=True)\n",
    "    try:\n",
    "        df = yf.download(\n",
    "            tickers=ticker,\n",
    "            interval=interval,\n",
    "            period=period,\n",
    "            progress=False,\n",
    "            threads=False,\n",
    "            auto_adjust=False\n",
    "        )\n",
    "        if df is None or df.empty:\n",
    "            print(\"No data returned from yfinance.\", flush=True)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "        df = df.loc[:, cols]\n",
    "        for c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        print(f\"Successfully fetched {len(df)} rows of data.\", flush=True)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data fetch: {e}\", flush=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def build_features_and_targets(df: pd.DataFrame, window: int, horizons: list):\n",
    "    print(\"Building features and targets...\", flush=True)\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for i in range(window, len(df) - max(horizons)):\n",
    "        window_slice = df.iloc[i - window : i]\n",
    "        close_prices = window_slice[\"Close\"]\n",
    "        volume = window_slice[\"Volume\"]\n",
    "\n",
    "        past_returns = close_prices.pct_change().dropna().values\n",
    "        if past_returns.ndim > 1:\n",
    "            past_returns = past_returns.flatten()\n",
    "\n",
    "        mean_return = np.mean(past_returns)\n",
    "        std_return = np.std(past_returns)\n",
    "        mean_volume = np.mean(volume)\n",
    "\n",
    "        current_date = df.index[i]\n",
    "        day_of_week = current_date.dayofweek / 6.0\n",
    "        month_of_year = current_date.month / 12.0\n",
    "\n",
    "        padded_past_returns = np.zeros(PAST_RETURNS_LENGTH)\n",
    "        actual_returns_length = min(len(past_returns), PAST_RETURNS_LENGTH)\n",
    "        padded_past_returns[-actual_returns_length:] = past_returns[-actual_returns_length:]\n",
    "\n",
    "        features = np.array([\n",
    "            mean_return,\n",
    "            std_return,\n",
    "            mean_volume,\n",
    "            day_of_week,\n",
    "            month_of_year,\n",
    "            *padded_past_returns\n",
    "        ])\n",
    "\n",
    "        current_price = df.iloc[i][\"Close\"]\n",
    "        future_returns = []\n",
    "        for h in horizons:\n",
    "            if i + h >= len(df):\n",
    "                future_returns.append(np.nan)\n",
    "            else:\n",
    "                future_price = df.iloc[i + h][\"Close\"]\n",
    "                future_return = (future_price - current_price) / (current_price + 1e-9)\n",
    "                future_returns.append(future_return)\n",
    "\n",
    "        if not any(np.isnan(future_returns)):\n",
    "             X_list.append(features)\n",
    "             y_list.append(future_returns)\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    y = y.reshape(y.shape[0], -1)\n",
    "    print(f\"Built X, y shapes: {X.shape}, {y.shape}\", flush=True)\n",
    "    return X, y\n",
    "\n",
    "def train_and_save(X, y):\n",
    "    print(\"Training new model...\", flush=True)\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise RuntimeError(f\"Shape mismatch: X={X.shape[0]}, y={y.shape[0]}\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, shuffle=False)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "\n",
    "    base_estimator = GradientBoostingRegressor(n_estimators=150, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "    model = MultiOutputRegressor(base_estimator, n_jobs=-1)\n",
    "    model.fit(X_train_s, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_val_s)\n",
    "    for i, h in enumerate(HORIZONS):\n",
    "        mae = mean_absolute_error(y_val[:, i], y_pred[:, i])\n",
    "        r2 = r2_score(y_val[:, i], y_pred[:, i])\n",
    "        print(f\"  - Horizon {h}-day => Validation MAE: {mae:.6f} | R²: {r2:.4f}\", flush=True)\n",
    "\n",
    "    joblib.dump(model, MODEL_FILE)\n",
    "    joblib.dump(scaler, SCALER_FILE)\n",
    "    print(f\"Saved model to {MODEL_FILE} and scaler to {SCALER_FILE}.\", flush=True)\n",
    "    return model, scaler\n",
    "\n",
    "def load_model_and_scaler():\n",
    "    if os.path.exists(MODEL_FILE) and os.path.exists(SCALER_FILE):\n",
    "        print(\"Loading existing model and scaler from disk.\", flush=True)\n",
    "        model = joblib.load(MODEL_FILE)\n",
    "        scaler = joblib.load(SCALER_FILE)\n",
    "        return model, scaler\n",
    "    return None, None\n",
    "\n",
    "def make_prediction_vector(df_recent: pd.DataFrame, window: int):\n",
    "    if len(df_recent) < window:\n",
    "        print(\"Not enough recent data to create a feature vector.\", flush=True)\n",
    "        return None\n",
    "\n",
    "    window_slice = df_recent.iloc[-window:]\n",
    "    close_prices = window_slice[\"Close\"]\n",
    "    volume = window_slice[\"Volume\"]\n",
    "\n",
    "    past_returns = close_prices.pct_change().dropna().values\n",
    "    if past_returns.ndim > 1:\n",
    "        past_returns = past_returns.flatten()\n",
    "\n",
    "    mean_return = np.mean(past_returns)\n",
    "    std_return = np.std(past_returns)\n",
    "    mean_volume = np.mean(volume)\n",
    "\n",
    "    current_date = df_recent.index[-1]\n",
    "    day_of_week = current_date.dayofweek / 6.0\n",
    "    month_of_year = current_date.month / 12.0\n",
    "\n",
    "    padded_past_returns = np.zeros(PAST_RETURNS_LENGTH)\n",
    "    actual_returns_length = min(len(past_returns), PAST_RETURNS_LENGTH)\n",
    "    padded_past_returns[-actual_returns_length:] = past_returns[-actual_returns_length:]\n",
    "\n",
    "    features = np.array([\n",
    "        mean_return,\n",
    "        std_return,\n",
    "        mean_volume,\n",
    "        day_of_week,\n",
    "        month_of_year,\n",
    "        *padded_past_returns\n",
    "    ]).reshape(1, -1)\n",
    "\n",
    "    return features\n",
    "\n",
    "def log_prediction(timestamp, ticker, pred_returns, last_price):\n",
    "    row = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"ticker\": ticker,\n",
    "        \"last_price\": float(last_price),\n",
    "    }\n",
    "    for i, h in enumerate(HORIZONS):\n",
    "        ret = pred_returns[i]\n",
    "        row[f\"pred_ret_{h}d\"] = float(ret)\n",
    "        row[f\"pred_price_{h}d\"] = float(last_price * (1 + ret))\n",
    "\n",
    "    df_log = pd.DataFrame([row])\n",
    "    header = not os.path.exists(LOG_FILE)\n",
    "    df_log.to_csv(LOG_FILE, mode=\"a\", header=header, index=False)\n",
    "    print(f\"Prediction logged to {LOG_FILE}\", flush=True)\n",
    "\n",
    "def main():\n",
    "    model, scaler = load_model_and_scaler()\n",
    "\n",
    "    if model is None or scaler is None:\n",
    "        df_hist = fetch_history(TICKER, INTERVAL, PERIOD)\n",
    "        if df_hist.empty:\n",
    "            print(\"Cannot train model without historical data. Exiting.\", flush=True)\n",
    "            return\n",
    "\n",
    "        X, y = build_features_and_targets(df_hist, WINDOW, HORIZONS)\n",
    "        if X.size == 0:\n",
    "            print(\"Failed to build features. Exiting.\", flush=True)\n",
    "            return\n",
    "\n",
    "        model, scaler = train_and_save(X, y)\n",
    "\n",
    "    print(\"\\nFetching latest data for prediction...\", flush=True)\n",
    "    df_recent = fetch_history(TICKER, INTERVAL, period=f\"{WINDOW + max(HORIZONS) + 5}d\")\n",
    "    if df_recent.empty or len(df_recent) < WINDOW + max(HORIZONS):\n",
    "        print(\"Not enough recent data to make a prediction. Exiting.\", flush=True)\n",
    "        return\n",
    "\n",
    "    feature_vector = make_prediction_vector(df_recent, WINDOW)\n",
    "    if feature_vector is None:\n",
    "        print(\"Could not create feature vector. Exiting.\", flush=True)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        fv_scaled = scaler.transform(feature_vector)\n",
    "        pred_returns = model.predict(fv_scaled)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\", flush=True)\n",
    "        return\n",
    "\n",
    "    last_close = df_recent.iloc[-1][\"Close\"]\n",
    "    now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50, flush=True)\n",
    "    print(f\"PREDICTION for {TICKER} @ {now_utc}\", flush=True)\n",
    "    print(f\"Based on last close price: {float(last_close):.2f}\", flush=True)\n",
    "    print(\"=\"*50, flush=True)\n",
    "\n",
    "    names = [\"Daily\", \"Weekly (5d)\", \"Monthly (21d)\"]\n",
    "    for i, h in enumerate(HORIZONS):\n",
    "        ret = pred_returns[i]\n",
    "        pred_price = float(last_close) * (1 + ret)\n",
    "        print(f\" {names[i]:<15} | Horizon: {h:2d} days | Return: {ret*100:+.2f}% | Predicted Price: {pred_price:.2f}\", flush=True)\n",
    "\n",
    "    log_prediction(now_utc, TICKER, pred_returns, float(last_close))\n",
    "\n",
    "    print(\"\\n--- Final Predictions ---\", flush=True)\n",
    "    print(f\"Daily Predicted Close Price: {float(last_close) * (1 + pred_returns[0]):.2f}\", flush=True)\n",
    "    print(f\"Weekly Predicted Close Price: {float(last_close) * (1 + pred_returns[1]):.2f}\", flush=True)\n",
    "    print(f\"Monthly Predicted Close Price: {float(last_close) * (1 + pred_returns[2]):.2f}\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fa400-41e2-4939-a512-a167dc29d65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bed00da-9850-4ddf-9f4e-0510eafa6a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 10y of 1d data for TATAMOTORS.BO...\n",
      "Successfully fetched 2443 rows of data.\n",
      "Building features and targets...\n",
      "Built X, y shapes: (2362, 35), (2362, 3)\n",
      "Training new model...\n",
      "  - Horizon 1-day => Validation MAE: 0.014971 | R²: -0.0968\n",
      "  - Horizon 5-day => Validation MAE: 0.035312 | R²: -0.0594\n",
      "  - Horizon 21-day => Validation MAE: 0.063527 | R²: -0.0829\n",
      "Saved model to model_tatamotors.bo_dwm.joblib and scaler to scaler_tatamotors.bo_dwm.joblib.\n",
      "\n",
      "Fetching latest data for prediction...\n",
      "Fetching 86d of 1d data for TATAMOTORS.BO...\n",
      "Successfully fetched 85 rows of data.\n",
      "\n",
      "==================================================\n",
      "PREDICTION for TATAMOTORS.BO @ 2025-09-21 09:37:38 UTC\n",
      "Based on last close price: 708.05\n",
      "==================================================\n",
      " Daily           | Horizon:  1 days | Return: +0.14% | Predicted Price: 709.06\n",
      " Weekly (5d)     | Horizon:  5 days | Return: -1.21% | Predicted Price: 699.49\n",
      " Monthly (21d)   | Horizon: 21 days | Return: +0.52% | Predicted Price: 711.70\n",
      "Prediction logged to logs/predictions_dwm.csv\n",
      "\n",
      "--- Final Predictions ---\n",
      "Daily Predicted Close Price: 709.06\n",
      "Weekly Predicted Close Price: 699.49\n",
      "Monthly Predicted Close Price: 711.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\3508052989.py:250: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Based on last close price: {float(last_close):.2f}\", flush=True)\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\3508052989.py:256: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  pred_price = float(last_close) * (1 + ret)\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\3508052989.py:259: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  log_prediction(now_utc, TICKER, pred_returns, float(last_close))\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\3508052989.py:262: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Daily Predicted Close Price: {float(last_close) * (1 + pred_returns[0]):.2f}\", flush=True)\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\3508052989.py:263: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Weekly Predicted Close Price: {float(last_close) * (1 + pred_returns[1]):.2f}\", flush=True)\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_18956\\3508052989.py:264: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Monthly Predicted Close Price: {float(last_close) * (1 + pred_returns[2]):.2f}\", flush=True)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "predict_dwm.py (Daily, Weekly, Monthly)\n",
    "\n",
    "- Uses daily bars (\"1d\") for long-term history.\n",
    "- Creates features from a look-back window and targets for 1-day, 5-day (weekly), and 21-day (monthly) returns.\n",
    "- Trains a MultiOutputRegressor to predict these three horizons simultaneously.\n",
    "- Runs once to generate the latest prediction and then exits.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# --------------- CONFIG ----------------\n",
    "TICKER = \"TATAMOTORS.BO\"\n",
    "INTERVAL = \"1d\"      # Changed to daily data\n",
    "PERIOD = \"10y\"       # Changed to a longer period for more context\n",
    "WINDOW = 60          # Look-back window in days\n",
    "HORIZONS = [1, 5, 21] # Prediction horizons: 1-day, 5-day (1 week), 21-day (1 month)\n",
    "PAST_RETURNS_LENGTH = 30 # Define a fixed length for past returns feature\n",
    "\n",
    "# --- File Paths ---\n",
    "MODEL_FILE = f\"model_{TICKER.lower()}_dwm.joblib\"\n",
    "SCALER_FILE = f\"scaler_{TICKER.lower()}_dwm.joblib\"\n",
    "LOG_FILE = \"logs/predictions_dwm.csv\"\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "# ---------------------------------------\n",
    "\n",
    "def fetch_history(ticker: str, interval: str, period: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetches and cleans historical market data.\"\"\"\n",
    "    print(f\"Fetching {period} of {interval} data for {ticker}...\", flush=True)\n",
    "    try:\n",
    "        df = yf.download(\n",
    "            tickers=ticker,\n",
    "            interval=interval,\n",
    "            period=period,\n",
    "            progress=False,\n",
    "            threads=False,\n",
    "            auto_adjust=False\n",
    "        )\n",
    "        if df is None or df.empty:\n",
    "            print(\"No data returned from yfinance.\", flush=True)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "        df = df.loc[:, cols]\n",
    "        for c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        print(f\"Successfully fetched {len(df)} rows of data.\", flush=True)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data fetch: {e}\", flush=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def build_features_and_targets(df: pd.DataFrame, window: int, horizons: list):\n",
    "    print(\"Building features and targets...\", flush=True)\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for i in range(window, len(df) - max(horizons)):\n",
    "        window_slice = df.iloc[i - window : i]\n",
    "        close_prices = window_slice[\"Close\"]\n",
    "        volume = window_slice[\"Volume\"]\n",
    "\n",
    "        past_returns = close_prices.pct_change().dropna().values\n",
    "        if past_returns.ndim > 1:\n",
    "            past_returns = past_returns.flatten()\n",
    "\n",
    "        mean_return = np.mean(past_returns)\n",
    "        std_return = np.std(past_returns)\n",
    "        mean_volume = np.mean(volume)\n",
    "\n",
    "        current_date = df.index[i]\n",
    "        day_of_week = current_date.dayofweek / 6.0\n",
    "        month_of_year = current_date.month / 12.0\n",
    "\n",
    "        padded_past_returns = np.zeros(PAST_RETURNS_LENGTH)\n",
    "        actual_returns_length = min(len(past_returns), PAST_RETURNS_LENGTH)\n",
    "        padded_past_returns[-actual_returns_length:] = past_returns[-actual_returns_length:]\n",
    "\n",
    "        features = np.array([\n",
    "            mean_return,\n",
    "            std_return,\n",
    "            mean_volume,\n",
    "            day_of_week,\n",
    "            month_of_year,\n",
    "            *padded_past_returns\n",
    "        ])\n",
    "\n",
    "        current_price = df.iloc[i][\"Close\"]\n",
    "        future_returns = []\n",
    "        for h in horizons:\n",
    "            if i + h >= len(df):\n",
    "                future_returns.append(np.nan)\n",
    "            else:\n",
    "                future_price = df.iloc[i + h][\"Close\"]\n",
    "                future_return = (future_price - current_price) / (current_price + 1e-9)\n",
    "                future_returns.append(future_return)\n",
    "\n",
    "        if not any(np.isnan(future_returns)):\n",
    "             X_list.append(features)\n",
    "             y_list.append(future_returns)\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    y = y.reshape(y.shape[0], -1)\n",
    "    print(f\"Built X, y shapes: {X.shape}, {y.shape}\", flush=True)\n",
    "    return X, y\n",
    "\n",
    "def train_and_save(X, y):\n",
    "    print(\"Training new model...\", flush=True)\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise RuntimeError(f\"Shape mismatch: X={X.shape[0]}, y={y.shape[0]}\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, shuffle=False)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "\n",
    "    base_estimator = GradientBoostingRegressor(n_estimators=150, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "    model = MultiOutputRegressor(base_estimator, n_jobs=-1)\n",
    "    model.fit(X_train_s, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_val_s)\n",
    "    for i, h in enumerate(HORIZONS):\n",
    "        mae = mean_absolute_error(y_val[:, i], y_pred[:, i])\n",
    "        r2 = r2_score(y_val[:, i], y_pred[:, i])\n",
    "        print(f\"  - Horizon {h}-day => Validation MAE: {mae:.6f} | R²: {r2:.4f}\", flush=True)\n",
    "\n",
    "    joblib.dump(model, MODEL_FILE)\n",
    "    joblib.dump(scaler, SCALER_FILE)\n",
    "    print(f\"Saved model to {MODEL_FILE} and scaler to {SCALER_FILE}.\", flush=True)\n",
    "    return model, scaler\n",
    "\n",
    "def load_model_and_scaler():\n",
    "    if os.path.exists(MODEL_FILE) and os.path.exists(SCALER_FILE):\n",
    "        print(\"Loading existing model and scaler from disk.\", flush=True)\n",
    "        model = joblib.load(MODEL_FILE)\n",
    "        scaler = joblib.load(SCALER_FILE)\n",
    "        return model, scaler\n",
    "    return None, None\n",
    "\n",
    "def make_prediction_vector(df_recent: pd.DataFrame, window: int):\n",
    "    if len(df_recent) < window:\n",
    "        print(\"Not enough recent data to create a feature vector.\", flush=True)\n",
    "        return None\n",
    "\n",
    "    window_slice = df_recent.iloc[-window:]\n",
    "    close_prices = window_slice[\"Close\"]\n",
    "    volume = window_slice[\"Volume\"]\n",
    "\n",
    "    past_returns = close_prices.pct_change().dropna().values\n",
    "    if past_returns.ndim > 1:\n",
    "        past_returns = past_returns.flatten()\n",
    "\n",
    "    mean_return = np.mean(past_returns)\n",
    "    std_return = np.std(past_returns)\n",
    "    mean_volume = np.mean(volume)\n",
    "\n",
    "    current_date = df_recent.index[-1]\n",
    "    day_of_week = current_date.dayofweek / 6.0\n",
    "    month_of_year = current_date.month / 12.0\n",
    "\n",
    "    padded_past_returns = np.zeros(PAST_RETURNS_LENGTH)\n",
    "    actual_returns_length = min(len(past_returns), PAST_RETURNS_LENGTH)\n",
    "    padded_past_returns[-actual_returns_length:] = past_returns[-actual_returns_length:]\n",
    "\n",
    "    features = np.array([\n",
    "        mean_return,\n",
    "        std_return,\n",
    "        mean_volume,\n",
    "        day_of_week,\n",
    "        month_of_year,\n",
    "        *padded_past_returns\n",
    "    ]).reshape(1, -1)\n",
    "\n",
    "    return features\n",
    "\n",
    "def log_prediction(timestamp, ticker, pred_returns, last_price):\n",
    "    row = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"ticker\": ticker,\n",
    "        \"last_price\": float(last_price),\n",
    "    }\n",
    "    for i, h in enumerate(HORIZONS):\n",
    "        ret = pred_returns[i]\n",
    "        row[f\"pred_ret_{h}d\"] = float(ret)\n",
    "        row[f\"pred_price_{h}d\"] = float(last_price * (1 + ret))\n",
    "\n",
    "    df_log = pd.DataFrame([row])\n",
    "    header = not os.path.exists(LOG_FILE)\n",
    "    df_log.to_csv(LOG_FILE, mode=\"a\", header=header, index=False)\n",
    "    print(f\"Prediction logged to {LOG_FILE}\", flush=True)\n",
    "\n",
    "def main():\n",
    "    model, scaler = load_model_and_scaler()\n",
    "\n",
    "    if model is None or scaler is None:\n",
    "        df_hist = fetch_history(TICKER, INTERVAL, PERIOD)\n",
    "        if df_hist.empty:\n",
    "            print(\"Cannot train model without historical data. Exiting.\", flush=True)\n",
    "            return\n",
    "\n",
    "        X, y = build_features_and_targets(df_hist, WINDOW, HORIZONS)\n",
    "        if X.size == 0:\n",
    "            print(\"Failed to build features. Exiting.\", flush=True)\n",
    "            return\n",
    "\n",
    "        model, scaler = train_and_save(X, y)\n",
    "\n",
    "    print(\"\\nFetching latest data for prediction...\", flush=True)\n",
    "    df_recent = fetch_history(TICKER, INTERVAL, period=f\"{WINDOW + max(HORIZONS) + 5}d\")\n",
    "    if df_recent.empty or len(df_recent) < WINDOW + max(HORIZONS):\n",
    "        print(\"Not enough recent data to make a prediction. Exiting.\", flush=True)\n",
    "        return\n",
    "\n",
    "    feature_vector = make_prediction_vector(df_recent, WINDOW)\n",
    "    if feature_vector is None:\n",
    "        print(\"Could not create feature vector. Exiting.\", flush=True)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        fv_scaled = scaler.transform(feature_vector)\n",
    "        pred_returns = model.predict(fv_scaled)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\", flush=True)\n",
    "        return\n",
    "\n",
    "    last_close = df_recent.iloc[-1][\"Close\"]\n",
    "    now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50, flush=True)\n",
    "    print(f\"PREDICTION for {TICKER} @ {now_utc}\", flush=True)\n",
    "    print(f\"Based on last close price: {float(last_close):.2f}\", flush=True)\n",
    "    print(\"=\"*50, flush=True)\n",
    "\n",
    "    names = [\"Daily\", \"Weekly (5d)\", \"Monthly (21d)\"]\n",
    "    for i, h in enumerate(HORIZONS):\n",
    "        ret = pred_returns[i]\n",
    "        pred_price = float(last_close) * (1 + ret)\n",
    "        print(f\" {names[i]:<15} | Horizon: {h:2d} days | Return: {ret*100:+.2f}% | Predicted Price: {pred_price:.2f}\", flush=True)\n",
    "\n",
    "    log_prediction(now_utc, TICKER, pred_returns, float(last_close))\n",
    "\n",
    "    print(\"\\n--- Final Predictions ---\", flush=True)\n",
    "    print(f\"Daily Predicted Close Price: {float(last_close) * (1 + pred_returns[0]):.2f}\", flush=True)\n",
    "    print(f\"Weekly Predicted Close Price: {float(last_close) * (1 + pred_returns[1]):.2f}\", flush=True)\n",
    "    print(f\"Monthly Predicted Close Price: {float(last_close) * (1 + pred_returns[2]):.2f}\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec56fd-db5f-4431-9e2f-38c5b932d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT STORED IN EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35652b3d-5e86-4bb9-877b-8fc6539b2f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   ---------------------------------------- 2/2 [openpyxl]\n",
      "\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3675c8d0-2c83-4827-b353-6ac504775e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model and scaler from disk.\n",
      "\n",
      "Fetching latest data for prediction...\n",
      "Fetching 86d of 1d data for TATAMOTORS.BO...\n",
      "Successfully fetched 85 rows of data.\n",
      "\n",
      "==================================================\n",
      "PREDICTION for TATAMOTORS.BO @ 2025-09-21 10:04:55 UTC\n",
      "Based on last close price: 708.05\n",
      "==================================================\n",
      " Daily           | Horizon:  1 days | Return: +0.14% | Predicted Price: 709.06\n",
      " Weekly (5d)     | Horizon:  5 days | Return: -1.21% | Predicted Price: 699.49\n",
      " Monthly (21d)   | Horizon: 21 days | Return: +0.52% | Predicted Price: 711.70\n",
      "Prediction logged to logs/predictions_dwm.csv\n",
      "Prediction saved to logs/predictions_dwm.xlsx\n",
      "\n",
      "--- Final Predictions ---\n",
      "Daily Predicted Close Price: 709.06\n",
      "Weekly Predicted Close Price: 699.49\n",
      "Monthly Predicted Close Price: 711.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_4116\\1876882424.py:273: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Based on last close price: {float(last_close):.2f}\", flush=True)\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_4116\\1876882424.py:279: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  pred_price = float(last_close) * (1 + ret)\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_4116\\1876882424.py:282: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  log_prediction(now_utc, TICKER, pred_returns, float(last_close))\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_4116\\1876882424.py:288: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  actual_price = float(df_recent.iloc[-h][\"Close\"])\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_4116\\1876882424.py:294: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  float(last_close) * (1 + pred_returns[0]),\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_4116\\1876882424.py:295: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  float(last_close) * (1 + pred_returns[1]),\n",
      "C:\\Users\\chara\\AppData\\Local\\Temp\\ipykernel_4116\\1876882424.py:296: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  float(last_close) * (1 + pred_returns[2]),\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "predict_dwm.py (Daily, Weekly, Monthly)\n",
    "\n",
    "- Uses daily bars (\"1d\") for long-term history.\n",
    "- Creates features from a look-back window and targets for 1-day, 5-day (weekly), and 21-day (monthly) returns.\n",
    "- Trains a MultiOutputRegressor to predict these three horizons simultaneously.\n",
    "- Runs once to generate the latest prediction and then exits.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import openpyxl\n",
    "\n",
    "# --------------- CONFIG ----------------\n",
    "TICKER = \"TATAMOTORS.BO\"\n",
    "INTERVAL = \"1d\"      # Daily data\n",
    "PERIOD = \"10y\"       # Longer history\n",
    "WINDOW = 60          # Look-back window in days\n",
    "HORIZONS = [1, 5, 21] # Prediction horizons\n",
    "PAST_RETURNS_LENGTH = 30\n",
    "\n",
    "# --- File Paths ---\n",
    "MODEL_FILE = f\"model_{TICKER.lower()}_dwm.joblib\"\n",
    "SCALER_FILE = f\"scaler_{TICKER.lower()}_dwm.joblib\"\n",
    "LOG_FILE = \"logs/predictions_dwm.csv\"\n",
    "EXCEL_FILE = \"logs/predictions_dwm.xlsx\"\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "# ---------------------------------------\n",
    "\n",
    "def fetch_history(ticker: str, interval: str, period: str) -> pd.DataFrame:\n",
    "    print(f\"Fetching {period} of {interval} data for {ticker}...\", flush=True)\n",
    "    try:\n",
    "        df = yf.download(\n",
    "            tickers=ticker,\n",
    "            interval=interval,\n",
    "            period=period,\n",
    "            progress=False,\n",
    "            threads=False,\n",
    "            auto_adjust=False\n",
    "        )\n",
    "        if df is None or df.empty:\n",
    "            print(\"No data returned from yfinance.\", flush=True)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "        df = df.loc[:, cols]\n",
    "        for c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        print(f\"Successfully fetched {len(df)} rows of data.\", flush=True)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data fetch: {e}\", flush=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def build_features_and_targets(df: pd.DataFrame, window: int, horizons: list):\n",
    "    print(\"Building features and targets...\", flush=True)\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for i in range(window, len(df) - max(horizons)):\n",
    "        window_slice = df.iloc[i - window : i]\n",
    "        close_prices = window_slice[\"Close\"]\n",
    "        volume = window_slice[\"Volume\"]\n",
    "\n",
    "        past_returns = close_prices.pct_change().dropna().values\n",
    "        if past_returns.ndim > 1:\n",
    "            past_returns = past_returns.flatten()\n",
    "\n",
    "        mean_return = np.mean(past_returns)\n",
    "        std_return = np.std(past_returns)\n",
    "        mean_volume = np.mean(volume)\n",
    "\n",
    "        current_date = df.index[i]\n",
    "        day_of_week = current_date.dayofweek / 6.0\n",
    "        month_of_year = current_date.month / 12.0\n",
    "\n",
    "        padded_past_returns = np.zeros(PAST_RETURNS_LENGTH)\n",
    "        actual_returns_length = min(len(past_returns), PAST_RETURNS_LENGTH)\n",
    "        padded_past_returns[-actual_returns_length:] = past_returns[-actual_returns_length:]\n",
    "\n",
    "        features = np.array([\n",
    "            mean_return,\n",
    "            std_return,\n",
    "            mean_volume,\n",
    "            day_of_week,\n",
    "            month_of_year,\n",
    "            *padded_past_returns\n",
    "        ])\n",
    "\n",
    "        current_price = df.iloc[i][\"Close\"]\n",
    "        future_returns = []\n",
    "        for h in horizons:\n",
    "            if i + h >= len(df):\n",
    "                future_returns.append(np.nan)\n",
    "            else:\n",
    "                future_price = df.iloc[i + h][\"Close\"]\n",
    "                future_return = (future_price - current_price) / (current_price + 1e-9)\n",
    "                future_returns.append(future_return)\n",
    "\n",
    "        if not any(np.isnan(future_returns)):\n",
    "            X_list.append(features)\n",
    "            y_list.append(future_returns)\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    y = y.reshape(y.shape[0], -1)\n",
    "    print(f\"Built X, y shapes: {X.shape}, {y.shape}\", flush=True)\n",
    "    return X, y\n",
    "\n",
    "def train_and_save(X, y):\n",
    "    print(\"Training new model...\", flush=True)\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise RuntimeError(f\"Shape mismatch: X={X.shape[0]}, y={y.shape[0]}\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, shuffle=False)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "\n",
    "    base_estimator = GradientBoostingRegressor(n_estimators=150, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "    model = MultiOutputRegressor(base_estimator, n_jobs=-1)\n",
    "    model.fit(X_train_s, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_val_s)\n",
    "    for i, h in enumerate(HORIZONS):\n",
    "        mae = mean_absolute_error(y_val[:, i], y_pred[:, i])\n",
    "        r2 = r2_score(y_val[:, i], y_pred[:, i])\n",
    "        print(f\"  - Horizon {h}-day => Validation MAE: {mae:.6f} | R²: {r2:.4f}\", flush=True)\n",
    "\n",
    "    joblib.dump(model, MODEL_FILE)\n",
    "    joblib.dump(scaler, SCALER_FILE)\n",
    "    print(f\"Saved model to {MODEL_FILE} and scaler to {SCALER_FILE}.\", flush=True)\n",
    "    return model, scaler\n",
    "\n",
    "def load_model_and_scaler():\n",
    "    if os.path.exists(MODEL_FILE) and os.path.exists(SCALER_FILE):\n",
    "        print(\"Loading existing model and scaler from disk.\", flush=True)\n",
    "        model = joblib.load(MODEL_FILE)\n",
    "        scaler = joblib.load(SCALER_FILE)\n",
    "        return model, scaler\n",
    "    return None, None\n",
    "\n",
    "def make_prediction_vector(df_recent: pd.DataFrame, window: int):\n",
    "    if len(df_recent) < window:\n",
    "        print(\"Not enough recent data to create a feature vector.\", flush=True)\n",
    "        return None\n",
    "\n",
    "    window_slice = df_recent.iloc[-window:]\n",
    "    close_prices = window_slice[\"Close\"]\n",
    "    volume = window_slice[\"Volume\"]\n",
    "\n",
    "    past_returns = close_prices.pct_change().dropna().values\n",
    "    if past_returns.ndim > 1:\n",
    "        past_returns = past_returns.flatten()\n",
    "\n",
    "    mean_return = np.mean(past_returns)\n",
    "    std_return = np.std(past_returns)\n",
    "    mean_volume = np.mean(volume)\n",
    "\n",
    "    current_date = df_recent.index[-1]\n",
    "    day_of_week = current_date.dayofweek / 6.0\n",
    "    month_of_year = current_date.month / 12.0\n",
    "\n",
    "    padded_past_returns = np.zeros(PAST_RETURNS_LENGTH)\n",
    "    actual_returns_length = min(len(past_returns), PAST_RETURNS_LENGTH)\n",
    "    padded_past_returns[-actual_returns_length:] = past_returns[-actual_returns_length:]\n",
    "\n",
    "    features = np.array([\n",
    "        mean_return,\n",
    "        std_return,\n",
    "        mean_volume,\n",
    "        day_of_week,\n",
    "        month_of_year,\n",
    "        *padded_past_returns\n",
    "    ]).reshape(1, -1)\n",
    "\n",
    "    return features\n",
    "\n",
    "def log_prediction(timestamp, ticker, pred_returns, last_price):\n",
    "    row = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"ticker\": ticker,\n",
    "        \"last_price\": float(last_price),\n",
    "    }\n",
    "    for i, h in enumerate(HORIZONS):\n",
    "        ret = pred_returns[i]\n",
    "        row[f\"pred_ret_{h}d\"] = float(ret)\n",
    "        row[f\"pred_price_{h}d\"] = float(last_price * (1 + ret))\n",
    "\n",
    "    df_log = pd.DataFrame([row])\n",
    "    header = not os.path.exists(LOG_FILE)\n",
    "    df_log.to_csv(LOG_FILE, mode=\"a\", header=header, index=False)\n",
    "    print(f\"Prediction logged to {LOG_FILE}\", flush=True)\n",
    "\n",
    "def save_to_excel(ticker, actual_prices, predicted_prices):\n",
    "    \"\"\"Save predictions to Excel with company name, actuals, and predictions.\"\"\"\n",
    "    row = {\n",
    "        \"Company\": ticker,\n",
    "        \"Actual_Daily\": actual_prices[0],\n",
    "        \"Actual_Weekly\": actual_prices[1],\n",
    "        \"Actual_Monthly\": actual_prices[2],\n",
    "        \"Predicted_Daily\": predicted_prices[0],\n",
    "        \"Predicted_Weekly\": predicted_prices[1],\n",
    "        \"Predicted_Monthly\": predicted_prices[2],\n",
    "    }\n",
    "\n",
    "    df_row = pd.DataFrame([row])\n",
    "\n",
    "    if not os.path.exists(EXCEL_FILE):\n",
    "        df_row.to_excel(EXCEL_FILE, index=False, engine=\"openpyxl\")\n",
    "    else:\n",
    "        with pd.ExcelWriter(EXCEL_FILE, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"overlay\") as writer:\n",
    "            startrow = writer.sheets[\"Sheet1\"].max_row\n",
    "            df_row.to_excel(writer, index=False, header=False, startrow=startrow)\n",
    "    print(f\"Prediction saved to {EXCEL_FILE}\", flush=True)\n",
    "\n",
    "def main():\n",
    "    model, scaler = load_model_and_scaler()\n",
    "\n",
    "    if model is None or scaler is None:\n",
    "        df_hist = fetch_history(TICKER, INTERVAL, PERIOD)\n",
    "        if df_hist.empty:\n",
    "            print(\"Cannot train model without historical data. Exiting.\", flush=True)\n",
    "            return\n",
    "\n",
    "        X, y = build_features_and_targets(df_hist, WINDOW, HORIZONS)\n",
    "        if X.size == 0:\n",
    "            print(\"Failed to build features. Exiting.\", flush=True)\n",
    "            return\n",
    "\n",
    "        model, scaler = train_and_save(X, y)\n",
    "\n",
    "    print(\"\\nFetching latest data for prediction...\", flush=True)\n",
    "    df_recent = fetch_history(TICKER, INTERVAL, period=f\"{WINDOW + max(HORIZONS) + 5}d\")\n",
    "    if df_recent.empty or len(df_recent) < WINDOW + max(HORIZONS):\n",
    "        print(\"Not enough recent data to make a prediction. Exiting.\", flush=True)\n",
    "        return\n",
    "\n",
    "    feature_vector = make_prediction_vector(df_recent, WINDOW)\n",
    "    if feature_vector is None:\n",
    "        print(\"Could not create feature vector. Exiting.\", flush=True)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        fv_scaled = scaler.transform(feature_vector)\n",
    "        pred_returns = model.predict(fv_scaled)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\", flush=True)\n",
    "        return\n",
    "\n",
    "    last_close = df_recent.iloc[-1][\"Close\"]\n",
    "    now_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50, flush=True)\n",
    "    print(f\"PREDICTION for {TICKER} @ {now_utc}\", flush=True)\n",
    "    print(f\"Based on last close price: {float(last_close):.2f}\", flush=True)\n",
    "    print(\"=\"*50, flush=True)\n",
    "\n",
    "    names = [\"Daily\", \"Weekly (5d)\", \"Monthly (21d)\"]\n",
    "    for i, h in enumerate(HORIZONS):\n",
    "        ret = pred_returns[i]\n",
    "        pred_price = float(last_close) * (1 + ret)\n",
    "        print(f\" {names[i]:<15} | Horizon: {h:2d} days | Return: {ret*100:+.2f}% | Predicted Price: {pred_price:.2f}\", flush=True)\n",
    "\n",
    "    log_prediction(now_utc, TICKER, pred_returns, float(last_close))\n",
    "\n",
    "    # Collect actual prices for daily, weekly, monthly\n",
    "    actual_prices = []\n",
    "    for h in HORIZONS:\n",
    "        if len(df_recent) >= h:\n",
    "            actual_price = float(df_recent.iloc[-h][\"Close\"])\n",
    "        else:\n",
    "            actual_price = float(last_close)\n",
    "        actual_prices.append(actual_price)\n",
    "\n",
    "    predicted_prices = [\n",
    "        float(last_close) * (1 + pred_returns[0]),\n",
    "        float(last_close) * (1 + pred_returns[1]),\n",
    "        float(last_close) * (1 + pred_returns[2]),\n",
    "    ]\n",
    "\n",
    "    save_to_excel(TICKER, actual_prices, predicted_prices)\n",
    "\n",
    "    print(\"\\n--- Final Predictions ---\", flush=True)\n",
    "    print(f\"Daily Predicted Close Price: {predicted_prices[0]:.2f}\", flush=True)\n",
    "    print(f\"Weekly Predicted Close Price: {predicted_prices[1]:.2f}\", flush=True)\n",
    "    print(f\"Monthly Predicted Close Price: {predicted_prices[2]:.2f}\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c08eea-5ed3-44cc-94cd-5a5b97e1c40d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
